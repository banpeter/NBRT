{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "#blob\n",
    "\n",
    "X_training = []\n",
    "y_training = []\n",
    "header = []\n",
    "c = 0\n",
    "with open(\"DataSets/blob_att.csv\",\"r\") as file:\n",
    "    r = csv.reader(file)\n",
    "    for row in r:\n",
    "        if(c == 0):\n",
    "            header = row\n",
    "            c+=1\n",
    "            continue\n",
    "        X_training.append([float(x) for x in row])\n",
    "        \n",
    "with open(\"DataSets/blob_label.csv\",\"r\") as file:\n",
    "    r = csv.reader(file)\n",
    "    for row in r:\n",
    "        y_training.append(int(row[0]))\n",
    "#print(X_training)\n",
    "#print(y_training)\n",
    "        \n",
    "        \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_training, y_training, test_size=0.2) # 80% training and 20% testing data\n",
    "#all_data.append([X_train, X_test, y_train, y_test,header])\n",
    "\n",
    "\n",
    "batch_x = []\n",
    "batch_y = []\n",
    "\n",
    "a = len(X_train)\n",
    "step = 1000\n",
    "\n",
    "for i in range(0, a, step):\n",
    "\n",
    "\n",
    "    #for j in range(i, i + step):\n",
    "        batch_x.append(X_train[0:i + step])\n",
    "        batch_y.append(y_train[0:i + step])\n",
    "print(len(batch_x))\n",
    "all_data.append([batch_x, X_test, batch_y, y_test,header])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris\n",
    "X_training = []\n",
    "y_training = []\n",
    "header = []\n",
    "c = 0\n",
    "with open(\"DataSets/iris.csv\",\"r\") as file:\n",
    "    r = csv.reader(file)\n",
    "    for row in r:\n",
    "        if(c == 0):\n",
    "            header = row[:-1]\n",
    "            c+=1\n",
    "            continue\n",
    "        X_training.append([float(x) for x in row[:-1]])\n",
    "        y_training.append(row[-1])\n",
    "        #print(row)\n",
    "\n",
    "for i in range(len(y_training)):\n",
    "    if(y_training[i] == \"Setosa\"):\n",
    "        y_training[i] = 1\n",
    "    if(y_training[i] == \"Versicolor\"):\n",
    "        y_training[i] = 2\n",
    "    if(y_training[i] == \"Virginica\"):\n",
    "        y_training[i] = 3\n",
    "        \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_training, y_training, test_size=0.2) # 80% training and 20% testing data\n",
    "#all_data.append([X_train, X_test, y_train, y_test,header])\n",
    "\n",
    "\n",
    "\n",
    "batch_x = []\n",
    "batch_y = []\n",
    "\n",
    "a = len(X_train)\n",
    "step = 10\n",
    "\n",
    "for i in range(0, a, step):\n",
    "\n",
    "\n",
    "    #for j in range(i, i + step):\n",
    "        batch_x.append(X_train[0:i + step])\n",
    "        batch_y.append(y_train[0:i + step])\n",
    "\n",
    "all_data.append([batch_x, X_test, batch_y, y_test,header])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fixed acidity', '\"volatile acidity\"', '\"citric acid\"', '\"residual sugar\"', '\"chlorides\"', '\"free sulfur dioxide\"', '\"total sulfur dioxide\"', '\"density\"', '\"pH\"', '\"sulphates\"', '\"alcohol\"']\n",
      "4898\n",
      "4898\n"
     ]
    }
   ],
   "source": [
    "#wine\n",
    "X_training = []\n",
    "y_training = []\n",
    "header = []\n",
    "c = 0\n",
    "with open(\"DataSets/winequality-white_att.csv\",\"r\") as file:\n",
    "    r = csv.reader(file)\n",
    "    for row in r:\n",
    "        if(c == 0):\n",
    "            header = row\n",
    "            c+=1\n",
    "            continue\n",
    "        X_training.append([float(x) for x in row[:-1]])\n",
    "        y_training.append(float(row[-1]))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_training, y_training, test_size=0.2) # 80% training and 20% testing data\n",
    "#all_data.append([X_train, X_test, y_train, y_test,header])\n",
    "print(header)\n",
    "print(len(X_training))\n",
    "print(len(y_training))\n",
    "\n",
    "\n",
    "\n",
    "batch_x = []\n",
    "batch_y = []\n",
    "\n",
    "a = len(X_train)\n",
    "step = 480\n",
    "\n",
    "for i in range(0, a, step):\n",
    "\n",
    "\n",
    "    #for j in range(i, i + step):\n",
    "        batch_x.append(X_train[0:i + step])\n",
    "        batch_y.append(y_train[0:i + step])\n",
    "\n",
    "all_data.append([batch_x, X_test, batch_y, y_test,header])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#house\n",
    "X_training = []\n",
    "y_training = []\n",
    "header = []\n",
    "c = 0\n",
    "with open(\"DataSets/house_att.csv\",\"r\") as file:\n",
    "    r = csv.reader(file)\n",
    "    for row in r:\n",
    "        if(c == 0):\n",
    "            header = row\n",
    "            c+=1\n",
    "            continue\n",
    "        X_training.append([float(x) for x in row])\n",
    "        \n",
    "with open(\"DataSets/house_price.csv\",\"r\") as file:\n",
    "    r = csv.reader(file)\n",
    "    for row in r:\n",
    "        y_training.append(float(row[0]))\n",
    "\n",
    "        \n",
    "print(len(X_training))\n",
    "print(len(y_training))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_training, y_training, test_size=0.2) # 80% training and 20% testing data\n",
    "#all_data.append([X_train, X_test, y_train, y_test,header])\n",
    "\n",
    "\n",
    "\n",
    "batch_x = []\n",
    "batch_y = []\n",
    "\n",
    "a = len(X_train)\n",
    "step = 1000\n",
    "\n",
    "for i in range(0, a, step):\n",
    "\n",
    "\n",
    "    #for j in range(i, i + step):\n",
    "        batch_x.append(X_train[0:i + step])\n",
    "        batch_y.append(y_train[0:i + step])\n",
    "print(len(batch_x))\n",
    "all_data.append([batch_x, X_test, batch_y, y_test,header])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort dataset by an attribute.(increasing order)\n",
    "def sort_f(X,y,column):\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        temp = i\n",
    "        for j in range(i,len(X)):\n",
    "            \n",
    "            #print(X[j])\n",
    "            #print(X[temp][column])\n",
    "            if(X[j][column]<X[temp][column]):\n",
    "                temp = j\n",
    "        if(temp != i):\n",
    "            tmp = X[i]\n",
    "            X[i] = X[temp]\n",
    "            X[temp] = tmp\n",
    "\n",
    "            tmp = y[i]\n",
    "            y[i] = y[temp]\n",
    "            y[temp] = tmp\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self,X,feature_names,labels):\n",
    "        \n",
    "        self.X = X\n",
    "        self.num_of_nodes = 0\n",
    "        self.currentsplit = 0\n",
    "        self.split_result = 0\n",
    "        self.feature_names = feature_names #coloum names\n",
    "        self.labels = labels#y\n",
    "        self.catagories = set(labels)\n",
    "        self.nodes = []\n",
    "        self.split = 0\n",
    "        self.leaf = 0\n",
    "        self.steps = [0,0,0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self,labels,X,feature_ids):\n",
    "        \n",
    "        self.split_result = 0\n",
    "        self.split = 0 #which column / which feature id\n",
    "        self.feature_ids = feature_ids\n",
    "        self.labels = labels\n",
    "        self.X = X\n",
    "        self.nodes = []\n",
    "        self.regr = 0\n",
    "        self.depth = 0\n",
    "        self.top = 0\n",
    "        self.bottom = 0\n",
    "        self.steps = [0,0,0]\n",
    "        self.prev_node = 0\n",
    "        \n",
    "        self.leaf = 0 #true or false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(X,labels,column,bottom,top):#return splited data\n",
    "    \n",
    "\n",
    "    #Return a portion of the dataset, which's attributes is in a certain\n",
    "    \n",
    "    sub_label = [labels[x] for x in range(len(labels)) if X[x][column]<top and X[x][column]>=bottom]\n",
    "    sub_X = [X[x] for x in range(len(X)) if X[x][column]<top and X[x][column]>=bottom]\n",
    "\n",
    "\n",
    "        \n",
    "    return [sub_X,sub_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(regr_type,X,y,column,dgr):\n",
    "    \n",
    "    X = [ [X[i][column]] for i in range(len(X)) ]\n",
    "\n",
    "    \n",
    "    regr = LinearRegression()\n",
    "    regr.fit(X,y)\n",
    "    \n",
    "    \n",
    "    return regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(regr,X,y,column,regr_type,dgr):\n",
    "    \n",
    "    \n",
    "\n",
    "    X = [ [X[i][column]] for i in range(len(X)) ]\n",
    "    if(regr_type == 1):\n",
    "        X = PolynomialFeatures(degree=dgr).fit_transform(X)\n",
    "    predict = regr.predict(X)\n",
    "\n",
    "    difference = []\n",
    "    \n",
    "    for i in range(len(predict)):\n",
    "        difference.append( (predict[i]-y[i])**2 )\n",
    "    error = sum(difference)/len(difference)\n",
    "\n",
    "  \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(i,j,column):\n",
    "    \n",
    "    \n",
    " \n",
    "    n1 = [ i[0][k][column] for k in range(len(i[0])) ]\n",
    "    n2 = [ j[0][k][column] for k in range(len(j[0])) ]\n",
    "    \n",
    "    \n",
    "    \n",
    "    Ni = len(n1)\n",
    "    Nj = len(n2)\n",
    "    \n",
    "    s1i = sum(n1)/Ni\n",
    "    s1j = sum(n2)/Nj\n",
    "    \n",
    "    s2i = sum([x**2 for x in n1])/Ni\n",
    "    s2j = sum([x**2 for x in n2])/Nj\n",
    "    \n",
    "    s3i = sum([x*y for x,y in zip(n1,i[1])])/Ni\n",
    "    s3j = sum([x*y for x,y in zip(n2,j[1])])/Nj\n",
    "    \n",
    "    s4i = sum(i[1])/Ni\n",
    "    s4j = sum(j[1])/Nj\n",
    "    \n",
    "    D = (s1i-s1j)**2 + (s2i-s2j)**2 + (s3i-s3j)**2 +(s4i-s4j)**2\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_distance(sliced_data,column):\n",
    "    \n",
    "    dist = 0\n",
    "    min_dist = -1\n",
    "    index1 = -1\n",
    "    index2 = -1\n",
    "    \n",
    "    #determine what to merge accordint to the linear regression\n",
    "    for i in range(len(sliced_data)-1):\n",
    "\n",
    "            dist = calculate_distance(sliced_data[i],sliced_data[i+1],column)\n",
    "            \n",
    "            if(min_dist == -1):\n",
    "                min_dist = dist\n",
    "                index1 = i\n",
    "                index2 = i+1\n",
    "            if(dist<min_dist):\n",
    "                min_dist = dist\n",
    "                index1 = i\n",
    "                index2 = i+1\n",
    "    \n",
    "    return index1,index2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(sliced_data,regressions,column):\n",
    "    \n",
    "   \n",
    "    index1,index2 = find_min_distance(sliced_data,column)\n",
    "    \n",
    "\n",
    "\n",
    "    sliced_data[index1][0] = sliced_data[index1][0]+sliced_data[index2][0]\n",
    "    sliced_data[index1][1] = sliced_data[index1][1]+sliced_data[index2][1]\n",
    "\n",
    "    regressions[index1] = regression(0,sliced_data[index1][0],sliced_data[index1][1],column,0)\n",
    "    \n",
    "    sliced_data.pop(index2)\n",
    "    regressions.pop(index2)\n",
    "    \n",
    "    return sliced_data,regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(X,labels,feature_ids,steps,div):\n",
    "    \n",
    "    step = -1\n",
    "    #print(steps)\n",
    "    features = 0 \n",
    "    svalue = 0\n",
    "    \n",
    "    min_error = -1\n",
    "    min_sliced_data = []\n",
    "    min_regressions = []\n",
    "    min_steps = -1\n",
    "    min_column = -1\n",
    "\n",
    "    for column in feature_ids:\n",
    "           \n",
    "            \n",
    "            X,labels = sort_f(X,labels,column)\n",
    "            features = [ X[j][column] for j in range(len(X)) ] \n",
    "            \n",
    "            sliced_data = []\n",
    "            regressions = []\n",
    "           \n",
    "            #determine the the range of the values\n",
    "            dist = abs(features[0]-features[-1])\n",
    "            \n",
    "            #defien a step size\n",
    "            step = int(dist/10)\n",
    "            \n",
    "            if(step < 2):\n",
    "                step = 1#minimum stepsize #az adott atributum legkisseb eleme...\n",
    "                #After the minimum steps size has benn reached,on an attribute,we not allow for infinite splits\n",
    "                if(steps[column] > 4):\n",
    "                    continue\n",
    "            #slice data according to step size\n",
    "            for k in range(int(features[0]),int(features[-1])+1,step): \n",
    "                \n",
    "                sliced_data.append(get_features(X,labels,column,k,k+step))\n",
    "                \n",
    "                if( len(sliced_data[-1][0]) == 0):\n",
    "                    \n",
    "                    \n",
    "                    sliced_data.pop(-1)\n",
    "                    continue\n",
    "                regressions.append( regression( 0,sliced_data[-1][0],sliced_data[-1][1],column,0 ) )\n",
    "            #merge the data\n",
    "            while(len(sliced_data)>div):\n",
    "               \n",
    "                sliced_data,regressions = merge(sliced_data,regressions,column)\n",
    "                \n",
    "            \n",
    "            error = 0\n",
    "            for i in range(len(sliced_data)):\n",
    "                \n",
    "                error += calculate_error(regressions[i],sliced_data[i][0],sliced_data[i][1],column,0,0)\n",
    "            \n",
    "            if(min_error == -1):\n",
    "                min_sliced_data = sliced_data\n",
    "                min_regressions = regressions\n",
    "                min_steps = step\n",
    "                min_column = column\n",
    "                min_error = error\n",
    "                continue\n",
    "            #choose the splited data with the minimum error\n",
    "            if(error<min_error):\n",
    "                min_sliced_data = sliced_data\n",
    "                min_regressions = regressions\n",
    "                min_steps = step\n",
    "                min_column = column\n",
    "    \n",
    "\n",
    "    if(min_steps < 2 and min_steps>=0):\n",
    "            steps[min_column] += 1\n",
    "    \n",
    "    return min_sliced_data,min_regressions,min_column,steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X,feature_ids,labels,leaf_size,limit,div,depth,steps):\n",
    "    \n",
    "    #find the best split for sub_nodes\n",
    "    #return sub_nodes\n",
    "\n",
    "    boundaries = [] \n",
    "    \n",
    "    \n",
    "    split = 0 \n",
    "    sub_nodes = []\n",
    "\n",
    "    st = steps.copy()\n",
    "    \n",
    "    #get the best split \n",
    "    split_data,split_regression,column,st = find_best_split(X,labels,feature_ids,st,div)\n",
    " \n",
    "    \n",
    "    for i,j in zip(split_data,split_regression):\n",
    "\n",
    "\n",
    "        node = Node(i[1],i[0],feature_ids)#elég lenne a határokat eltárolni\n",
    "        node.split = column   \n",
    "\n",
    " \n",
    "\n",
    "        node.split_result = i[0][-1][column]\n",
    "        node.depth = depth\n",
    "        node.regr = j\n",
    "\n",
    "        node.top = i[0][-1][column]\n",
    "        node.bottom = i[0][0][column]\n",
    "        node.steps = st\n",
    "\n",
    "\n",
    "        \n",
    "        if(len(node.labels) <= leaf_size or calculate_error(node.regr,node.X,node.labels,split,0,3)<limit):\n",
    "            #print(len(node.labels))\n",
    "            #print(calculate_error(node.regr,node.X,node.labels,split,0,3))\n",
    "            node.leaf = 1\n",
    "\n",
    "\n",
    "        else:\n",
    "            node.leaf = 0\n",
    "        sub_nodes.append(node)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(depth == 2000):\n",
    "        for i in sub_nodes:\n",
    "            i.leaf = 1\n",
    "            print(\"leaf2\")\n",
    "        return sub_nodes\n",
    "    \n",
    "    depth +=1\n",
    "\n",
    "    #print(depth)\n",
    "    leaf =  0\n",
    "    for i in sub_nodes:\n",
    "\n",
    "        if(i.leaf == 1):\n",
    "            leaf +=1\n",
    "\n",
    "            \n",
    "            \n",
    "    if(leaf == len(sub_nodes) and leaf != 0):\n",
    "\n",
    "        return sub_nodes\n",
    "    \n",
    "    else:\n",
    "        for node in sub_nodes:\n",
    "            if(node.leaf == 0):\n",
    "                    node.nodes = build_tree(node.X,node.feature_ids,node.labels,leaf_size,limit,div,depth,node.steps)\n",
    "   \n",
    "\n",
    "               \n",
    "\n",
    "    return sub_nodes   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def rebuild_tree(X,feature_ids,labels,leaf_size,limit,div,depth,steps,prev_node,amount):\n",
    "    \n",
    "    #Két felé bontjuk az adatokat úgy hogy a legkisseb mse kapjuk\n",
    "    #print(prev_node.depth)\n",
    "    boundaries = [] # last element of each bach\n",
    "    \n",
    "    \n",
    "    \n",
    "    split = 0 \n",
    "    sub_nodes = []\n",
    "    \n",
    "    st = steps.copy()\n",
    "    \n",
    "    \n",
    "    split_data,split_regression,column,st = find_best_split(X,labels,feature_ids,st,div)\n",
    "    \n",
    "    \n",
    "    rebuild = 0\n",
    "    err_old = 0\n",
    "    err_new = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if(len(prev_node.nodes) == len(split_data)):\n",
    "\n",
    "            for i in range(len(prev_node.nodes)):\n",
    "                err_old += calculate_error(prev_node.nodes[i].regr, prev_node.nodes[i].X,prev_node.nodes[i].labels,\n",
    "                                                prev_node.nodes[i].split,0,3)\n",
    "                \n",
    "                err_new += calculate_error(prev_node.nodes[i].regr, split_data[i][0], split_data[i][1],\n",
    "                                                column, 0, 3)\n",
    "\n",
    "            \n",
    "            for i in range(len(split_data)):\n",
    "                err_new += calculate_error(split_regression[i], split_data[i][0], split_data[i][1],\n",
    "                                                column, 0, 3)\n",
    "                err_old += calculate_error(prev_node.nodes[i].regr, split_data[i][0], split_data[i][1],\n",
    "                                                    column, 0, 3)\n",
    "    \n",
    "    else:\n",
    "        #print(\"in2\")\n",
    "        #print(len(prev_node.nodes))\n",
    "        #print(len(split_data))\n",
    "        for i in range(len(prev_node.nodes)):\n",
    "            err_old += calculate_error(prev_node.nodes[i].regr, prev_node.nodes[i].X,prev_node.nodes[i].labels,\n",
    "                                            prev_node.nodes[i].split,0,3)\n",
    "\n",
    "    \n",
    "        for i in range(len(split_data)):\n",
    "            err_new += calculate_error(split_regression[i], split_data[i][0], split_data[i][1],\n",
    "                                            column, 0, 3)\n",
    "\n",
    "        #print(err_old)\n",
    "        #print(err_new)\n",
    "    \n",
    "    #diff = err_new/err_old\n",
    "    min_err = min(err_new,err_old)\n",
    "    #print(diff)\n",
    "\n",
    "    if(err_new<err_old):\n",
    "        rebuild = 1\n",
    "        #rebuild\n",
    "    # L = l_rate*(err_new/err_old)*time\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(rebuild == 1 and len(X) > 5000):\n",
    "        \n",
    "        for node in prev_node.nodes:\n",
    "                if(node.leaf == 0):\n",
    "                    node.nodes = rebuild_tree(node.X,node.feature_ids,node.labels,leaf_size,limit,div,depth,node.steps,node,amount)\n",
    "        \n",
    "                 \n",
    "    else:\n",
    "        \n",
    "        old_tresholds = []\n",
    "        new_tresholds = []\n",
    "        threshold = []\n",
    "        \n",
    "        L = len(X)/amount\n",
    "\n",
    "        for i in range(len(prev_node.nodes)):\n",
    "\n",
    "            arr = prev_node.nodes[i].X\n",
    "            col = prev_node.nodes[i].split\n",
    "            arr = [ [arr[x][column]] for x in range(len(arr)) ]\n",
    "            old_tresholds.append([min(arr),max(arr)])\n",
    "\n",
    "        for i in range(len(split_data)):\n",
    "\n",
    "            arr = split_data[i][0]\n",
    "            arr = [ [arr[x][column]] for x in range(len(arr)) ]\n",
    "            new_tresholds.append([min(arr),max(arr)])\n",
    "\n",
    "\n",
    "        for i,j in zip(old_tresholds,new_tresholds):\n",
    "\n",
    "            bottom = (i[0][0]+j[0][0])/2\n",
    "            \n",
    "            top = (i[1][0]+j[1][0])/2\n",
    "\n",
    "            threshold.append([bottom,top])\n",
    "        \n",
    "        #split_data = []\n",
    "        for i in threshold: \n",
    "                \n",
    "                split_data.append(get_features(X,labels,column,i[0],i[1]))\n",
    "        \n",
    "                #regressions.append( regression( 0,sliced_data[-1][0],sliced_data[-1][1],column,0 ) )\n",
    "        c = 0\n",
    "        print(threshold)\n",
    "        if(threshold != []):\n",
    "            for node in prev_node.nodes:\n",
    "                node.bottom = threshold[c][0]\n",
    "                node.top = threshold[c][1]\n",
    "                c+=1\n",
    "        \n",
    "        c = 0\n",
    "        for node in prev_node.nodes:\n",
    "                if(node.leaf == 0):\n",
    "\n",
    "                    node.nodes = rebuild_tree(split_data[c][0],prev_node.feature_ids,split_data[c][1],leaf_size,limit,div,prev_node.depth,prev_node.steps,node,amount)\n",
    "                    c += 1\n",
    "                \n",
    "    return prev_node.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicialize(X,feature_names,labels,leaf_size,limit,div,depth,prev_nodes,rebuild,amount):\n",
    "    \n",
    "\n",
    "    feature_ids = [x for x in range(len(feature_names))]\n",
    "    tree = Tree(X,feature_names,labels)\n",
    "    steps = []\n",
    "    for i in feature_ids:\n",
    "        steps.append(0)\n",
    "\n",
    "    if(rebuild == 1):\n",
    "        #itt kéne meghatározni a learning ratet\n",
    "        for i in prev_nodes.nodes:\n",
    "            #print(i.nodes)\n",
    "            i.nodes = rebuild_tree(X,feature_ids,labels,leaf_size,limit,div,depth,steps,i,amount)\n",
    "        tree.nodes = prev_nodes.nodes\n",
    "        \n",
    "    else:\n",
    "        tree.nodes = build_tree(X,feature_ids,labels,leaf_size,limit,div,depth,steps)\n",
    "   \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "-------------------------------------------------------\n",
      "[[-8.478353185202003, -1.9072321731881086], [-0.25104145858290794, 6.07043294710715]]\n",
      "[[-6.4922695598401035, -1.9212753297784828], [-4.180334970973491, 0.827091279458581]]\n",
      "[[-1.930720691726953, 2.371035754900099], [0.30390705386966466, 6.111161406630919]]\n",
      "[[-5.2031680595141605, -2.5259378792238505], [-5.4862154095568805, -2.5002518594974887]]\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-859-81bc38404f0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;31m#print(\"....................................................\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[1;31m#print(tree.nodes)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                 \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minicialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mamount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-858-033a9e1e5bd3>\u001b[0m in \u001b[0;36minicialize\u001b[1;34m(X, feature_names, labels, leaf_size, limit, div, depth, prev_nodes, rebuild, amount)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprev_nodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;31m#print(i.nodes)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrebuild_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleaf_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mamount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprev_nodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-857-e664854b069e>\u001b[0m in \u001b[0;36mrebuild_tree\u001b[1;34m(X, feature_ids, labels, leaf_size, limit, div, depth, steps, prev_node, amount)\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaf\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                     \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrebuild_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleaf_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mamount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m                     \u001b[0mc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-857-e664854b069e>\u001b[0m in \u001b[0;36mrebuild_tree\u001b[1;34m(X, feature_ids, labels, leaf_size, limit, div, depth, steps, prev_node, amount)\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaf\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                     \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrebuild_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleaf_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mamount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m                     \u001b[0mc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-857-e664854b069e>\u001b[0m in \u001b[0;36mrebuild_tree\u001b[1;34m(X, feature_ids, labels, leaf_size, limit, div, depth, steps, prev_node, amount)\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaf\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                     \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrebuild_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleaf_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mamount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m                     \u001b[0mc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-857-e664854b069e>\u001b[0m in \u001b[0;36mrebuild_tree\u001b[1;34m(X, feature_ids, labels, leaf_size, limit, div, depth, steps, prev_node, amount)\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaf\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                     \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrebuild_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleaf_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mamount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m                     \u001b[0mc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-857-e664854b069e>\u001b[0m in \u001b[0;36mrebuild_tree\u001b[1;34m(X, feature_ids, labels, leaf_size, limit, div, depth, steps, prev_node, amount)\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaf\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                     \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrebuild_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msplit_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleaf_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprev_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mamount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m                     \u001b[0mc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "forest = []\n",
    "times = []\n",
    "divide = [2,3,4,5]\n",
    "\n",
    "\n",
    "amount = 0\n",
    "for i in all_data:\n",
    "    f = []\n",
    "    t = []\n",
    "    for k in divide:\n",
    "            print(k)\n",
    "            print(\"-------------------------------------------------------\")\n",
    "\n",
    "            start = time.time()\n",
    "            amount+=len(i[0][0])\n",
    "            tree = inicialize(i[0][0],i[-1],i[2][0],10,0.1,k,1,0,0,amount)\n",
    "            #print(tree)\n",
    "            \n",
    "            \n",
    "            for u,v in zip(i[0][1:],i[2][1:]): \n",
    "                #print(\"....................................................\")\n",
    "                #print(tree.nodes)\n",
    "                tree = inicialize(u,i[-1],v,10,1,k,1,tree,1,amount)\n",
    "\n",
    "            end = time.time()\n",
    "            \n",
    "            f.append(tree)\n",
    "            print(end-start)\n",
    "            t.append(end-start)\n",
    "            \n",
    "            \n",
    "    forest.append(f)\n",
    "    times.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(node,value):\n",
    "\n",
    "    if(node.leaf == 1 ):#\n",
    "        p = node.regr.predict(np.array(value[node.split]).reshape((1,-1)))\n",
    "\n",
    "        \n",
    "        return p\n",
    "    else:\n",
    "\n",
    "        c=0\n",
    "        for i in node.nodes:\n",
    "           \n",
    "            if (i.top>=value[i.split] and i.bottom<=value[i.split]):\n",
    "               \n",
    "                c+=1\n",
    "                p=predict(i,value)\n",
    "                return p\n",
    "        if(c == 0):\n",
    "            try:\n",
    "                p = node.regr.predict(np.array(value[node.split]).reshape((1,-1)))\n",
    "                return p\n",
    "            except:\n",
    "                return [-1]\n",
    "            \n",
    "            print(node.split)\n",
    "            print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predicted_value = []#predicted points\n",
    "difference = []\n",
    "MSE = []\n",
    "sum_y = []\n",
    "\n",
    "#all_difference = []\n",
    "\n",
    "#for each tree we calculate the mean squared error\n",
    "#all_data = [X_train,X_test,y_train,y_test,header]\n",
    "all_mse = []\n",
    "      \n",
    "for v in range(len(forest)):\n",
    "    MSE = []\n",
    "    \n",
    "    for i in forest[v]:\n",
    "        \n",
    "        j = all_data[v]\n",
    "        \n",
    "        X_test = j[1]\n",
    "        y_test = j[3]\n",
    "        pv = []\n",
    "        error = 0\n",
    "        mse = 0\n",
    "        for k in range(len(X_test)): \n",
    "            \n",
    "            p = predict(i,X_test[k])\n",
    "\n",
    "            if(p == None):\n",
    "                pv.append(0)\n",
    "                continue\n",
    "           \n",
    "            if(abs(y_test[k]-p[0])<1000 and p[0]>0):\n",
    "                error += abs(y_test[k]-p)\n",
    "                mse += (y_test[k]-p)**2\n",
    "                pv.append(p)\n",
    "            else:\n",
    "                #print(\"afa\")\n",
    "                pv.append(0)\n",
    "\n",
    "\n",
    "        sum_y.append(sum(y_test))\n",
    "        predicted_value.append(pv)\n",
    "        difference.append(error)\n",
    "        \n",
    "        MSE.append(mse[0]/len(X_test))\n",
    "    print(MSE)\n",
    "    all_mse.append(MSE)\n",
    "    \n",
    "    \n",
    "print(difference)\n",
    "#print(sum_y)\n",
    "#print(all_mse)\n",
    "\n",
    "with open(\"DataSets/difference_rtni2.csv\", 'a',newline='') as f:\n",
    "    # create the csv writer\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write a row to the csv file\n",
    "    for row in all_mse:\n",
    "        writer.writerow(row)\n",
    "        \n",
    "with open(\"DataSets/times_rtni2.csv\", 'a',newline='') as f:\n",
    "    # create the csv writer\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write a row to the csv file\n",
    "    #print(times)\n",
    "    for row in times:\n",
    "        \n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
