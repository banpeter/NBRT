import numpy as np
import matplotlib.pyplot as plt
import random
import math
from sklearn.model_selection import train_test_split
from sklearn import linear_model
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
import csv


#blob

X_training = []
y_training = []
header = []
c = 0
with open("DataSets/blobs_att.csv","r") as file:
    r = csv.reader(file)
    for row in r:
        if(c == 0):
            header = row[:-1]
            c+=1
            continue
        X_training.append([float(x) for x in row])
        
with open("DataSets/blobs_label.csv","r") as file:
    r = csv.reader(file)
    for row in r:
        y_training.append(int(row[0]))#row[0]!!!
print(X_training)
print(y_training)
        
        
X_train, X_test, y_train, y_test = train_test_split(X_training, y_training, test_size=0.2) # 80% training and 20% testing data
all_data.append([X_train, X_test, y_train, y_test,header])


#iris
X_training = []
y_training = []
header = []
c = 0
with open("DataSets/iris.csv","r") as file:
    r = csv.reader(file)
    for row in r:
        if(c == 0):
            header = row[:-1]
            c+=1
            continue
        X_training.append([float(x) for x in row[:-1]])
        y_training.append(row[-1])
        #print(row)

for i in range(len(y_training)):
    if(y_training[i] == "Setosa"):
        y_training[i] = 1
    if(y_training[i] == "Versicolor"):
        y_training[i] = 2
    if(y_training[i] == "Virginica"):
        y_training[i] = 3
        
X_train, X_test, y_train, y_test = train_test_split(X_training, y_training, test_size=0.2) # 80% training and 20% testing data
all_data.append([X_train, X_test, y_train, y_test,header])



#wine
X_training = []
y_training = []
header = []
c = 0
with open("DataSets/winequality-white_att.csv","r") as file:
    r = csv.reader(file)
    for row in r:
        if(c == 0):
            header = row
            c+=1
            continue
        X_training.append([float(x) for x in row[:-1]])
        y_training.append(float(row[-1]))
        #print(row)
X_train, X_test, y_train, y_test = train_test_split(X_training, y_training, test_size=0.2) # 80% training and 20% testing data
all_data.append([X_train, X_test, y_train, y_test,header])
print(header)
print(X_training)
print(y_training)


#house
X_training = []
y_training = []
header = []
c = 0
with open("DataSets/house_att.csv","r") as file:
    r = csv.reader(file)
    for row in r:
        if(c == 0):
            header = row[:-1]
            c+=1
            continue
        X_training.append([float(x) for x in row])
        
with open("DataSets/house_price.csv","r") as file:
    r = csv.reader(file)
    for row in r:
        y_training.append(float(row[0]))#row[0]!!!

X_train, X_test, y_train, y_test = train_test_split(X_training, y_training, test_size=0.2) # 80% training and 20% testing data
all_data.append([X_train, X_test, y_train, y_test,header])
print(X_training)
print(y_training)



#Ház ár legenerálása
#Egy mérethez egy árat számítunk


#size: square meter math.sqrt(i+j/10)*7+ 0.2*i)*1000000
#distance from the center 1000/distance +10   dinstance 100-5000 meter
#interest rate 1-10% = -(x**2)/7 +30
#government support yes = 0-10 million
#local average salaray(HUF) 2-8


X_training = []
y_training = []
#header = ["size","support","salary"]
header = ["size","distance","support","salary"]

for i in range(10000):
    
    price = 0
    
    size = random.randint(0,99)+random.randint(0,10)/10
    distance = random.randint(30,100)
    interest_rate = random.randint(1,10)
    support = random.randint(0,10)
    salary = random.randint(3,8)

    price += ((math.sqrt(size)*10+0.3*(size/10)))
    price += (1000/distance + 5)
    #price += (-(interest_rate**2)/4 +30)
    price += support
    price += salary**3
    

    #X_training.append([size,support,salary])
    X_training.append([size,distance,support,salary])
    y_training.append(price)
    
X_train = []
y_train = []

X_test = []
y_test = []

#Felosztjuk az adatokat 
X_train, X_test, y_train, y_test = train_test_split(X_training, y_training, test_size=0.2) # 80% training and 20% testing data
print(X_train[0])
print(y_train[0])




#Sorba rendezzük az adathalmazt a méret alapján növekvően
def sort_f(X,y,column):
    
    for i in range(len(X)-1):
        temp = i
        for j in range(i,len(X)):
            
            
            #print(X[temp][column])
            if(X[j][column]<X[temp][column]):
                temp = j
        if(temp != i):
            tmp = X[i]
            X[i] = X[temp]
            X[temp] = tmp

            tmp = y[i]
            y[i] = y[temp]
            y[temp] = tmp
    return X,y





# visualize data set
plt.rcParams["figure.figsize"] = (15,7)
plt.scatter([ X_training[i][0] for i in range(len(X_training)) ],y_training,s=1)

plt.xlabel("Square meter")
plt.ylabel("Price")
plt.title("All Data")

plt.show()




plt.rcParams["figure.figsize"] = (15,7)
plt.scatter([ X_training[i][1] for i in range(len(X_training)) ],y_training,s=1)

plt.xlabel("Square meter")
plt.ylabel("Price")
plt.title("All Data")

plt.show()


# visualize data set
plt.rcParams["figure.figsize"] = (15,7)
plt.scatter([ X_test[i][0] for i in range(len(X_test)) ],y_test,s=1)

plt.xlabel("Square meter")
plt.ylabel("Price")
plt.title("Test Data")

plt.show()

class Tree:
    def __init__(self,X,feature_names,labels):
        
        self.X = X
        self.num_of_nodes = 0
        self.currentsplit = 0
        self.split_result = 0
        self.feature_names = feature_names #coloum names
        self.labels = labels#y
        self.catagories = set(labels)
        self.nodes = []
        self.split = 0
        self.leaf = 0
        self.steps = [0,0,0]
        
class Node:
    def __init__(self,labels,X,feature_ids):
        
        self.split_result = 0
        self.split = 0 #which column / which feature id
        self.feature_ids = feature_ids
        self.labels = labels
        self.X = X
        self.nodes = []
        self.regr = 0
        self.depth = 0
        self.top = 0
        self.bottom = 0
        self.steps = [0,0,0]
        
        self.leaf = 0 #true or false

def get_features(X,labels,column,bottom,top):#return splited data
    
    #on which dimension it should split the data -> column

    #Itt az adatokat mindig kettő felé osztjuk
    
    #number of dimensions
    #features = [ X[i][column] for i in range(bottom,top) ]#take a column
    #features = [ X[i][column] for i in range(len(X))  if X[i][column]<top and X[i][column]>=bottom]
    #print(features)
    
    ############################################################################
    sub_label = [labels[x] for x in range(len(labels)) if X[x][column]<top and X[x][column]>=bottom]
    sub_X = [X[x] for x in range(len(X)) if X[x][column]<top and X[x][column]>=bottom]
    #############################################################################
    #print(bottom)
    #print(top)
    
    #print(sub_X)
    #sub_label = [labels[x] for x in range(bottom,top)]
    #sub_X = [X[x] for x in range(bottom,top)]

        
    return [sub_X,sub_label]



def regression(regr_type,X,y,column,dgr):#whaT TYPE OF REGRESSION,data
    
    
    #reshape data
    
    X = [ [X[i][column]] for i in range(len(X)) ]
    #y = np.array(y).reshape((-1,1))
    #print(X)
    #print(y)
    
    regr = LinearRegression()
    regr.fit(X,y)
    
    
    return regr

def calculate_error(regr,X,y,column,regr_type,dgr):
    
    
    #reshape data
    X = [ [X[i][column]] for i in range(len(X)) ]
    if(regr_type == 1):
        X = PolynomialFeatures(degree=dgr).fit_transform(X)
    predict = regr.predict(X)

    difference = []
    
    for i in range(len(predict)):
        difference.append( (predict[i]-y[i])**2 )
    error = sum(difference)/len(difference)

  
    return error



def calculate_distance(i,j,column):
    
    #print("///////////////////////")
    #print(i)
 
    n1 = [ i[0][k][column] for k in range(len(i[0])) ]
    n2 = [ j[0][k][column] for k in range(len(j[0])) ]
    
    
    
    Ni = len(n1)
    Nj = len(n2)
    
    s1i = sum(n1)/Ni
    s1j = sum(n2)/Nj
    
    s2i = sum([x**2 for x in n1])/Ni
    s2j = sum([x**2 for x in n2])/Nj
    
    s3i = sum([x*y for x,y in zip(n1,i[1])])/Ni
    s3j = sum([x*y for x,y in zip(n2,j[1])])/Nj
    
    s4i = sum(i[1])/Ni
    s4j = sum(j[1])/Nj
    
    D = (s1i-s1j)**2 + (s2i-s2j)**2 + (s3i-s3j)**2 +(s4i-s4j)**2
    
    return D


def find_min_distance(sliced_data,column):
    
    dist = 0
    min_dist = -1
    index1 = -1
    index2 = -1
    #print(sliced_data[1])
    
    for i in range(len(sliced_data)-1):

            dist = calculate_distance(sliced_data[i],sliced_data[i+1],column)
            
            if(min_dist == -1):
                min_dist = dist
                index1 = i
                index2 = i+1
            if(dist<min_dist):
                min_dist = dist
                index1 = i
                index2 = i+1
    
    return index1,index2
   


def merge(sliced_data,regressions,column):
    
   
    #print(sliced_data)
    #print("////////////////////////////////////////////////////////////////////////////")
    index1,index2 = find_min_distance(sliced_data,column)
    
    #print(sliced_data)

    sliced_data[index1][0] = sliced_data[index1][0]+sliced_data[index2][0]
    sliced_data[index1][1] = sliced_data[index1][1]+sliced_data[index2][1]

    regressions[index1] = regression(0,sliced_data[index1][0],sliced_data[index1][1],column,0)
    #print("###############################################################################")
    sliced_data.pop(index2)
    regressions.pop(index2)
    
    return sliced_data,regressions


def find_best_split(X,labels,feature_ids,steps):
    
    step = -1
    #print(steps)
    features = 0 
    svalue = 0
    
    min_error = -1
    min_sliced_data = []
    min_regressions = []
    min_steps = -1
    min_column = -1

    for column in feature_ids:
            #print(column)
            
            X,labels = sort_f(X,labels,column)
            features = [ X[j][column] for j in range(len(X)) ] 
            
            sliced_data = []
            regressions = []
           
            
            dist = abs(features[0]-features[-1])
            
            
            step = int(dist/10)
            
            if(step < 2):
                step = 1
                #print(steps[column])
                #print(column)
                a = [2,3]
                b = [0,1]
                if(steps[column] > 1 ):
                    continue
            #print("//////////////////////////////////////////////////////")
            for k in range(int(features[0]),int(features[-1])+1,step): 
                #print(k)
                sliced_data.append(get_features(X,labels,column,k,k+step))
                
                if( len(sliced_data[-1][0]) == 0):
                    #print("zero")
                    #print(step)
                    
                    sliced_data.pop(-1)
                    continue
                regressions.append( regression( 0,sliced_data[-1][0],sliced_data[-1][1],column,0 ) )
                
            while(len(sliced_data)>4):
                #print("merge")
                sliced_data,regressions = merge(sliced_data,regressions,column)
                
            
            error = 0
            for i in range(len(sliced_data)):
                
                error += calculate_error(regressions[i],sliced_data[i][0],sliced_data[i][1],column,0,0)
            #sum the error after the split
            #choose min
            
            if(min_error == -1):
                min_sliced_data = sliced_data
                min_regressions = regressions
                min_steps = step
                min_column = column
                min_error = error
                continue
                
            if(error<min_error):
                min_sliced_data = sliced_data
                min_regressions = regressions
                min_steps = step
                min_column = column
    
    #return split,split_value
    #print(len(min_sliced_data))
    if(min_steps < 2 and min_steps>=0):
            steps[min_column] += 1
    
    return min_sliced_data,min_regressions,min_column,steps


def build_tree(X,feature_ids,labels,leaf_size,var,depth,steps):
    
    #Két felé bontjuk az adatokat úgy hogy a legkisseb mse kapjuk

    boundaries = [] # last element of each bach
    
    
    
    split = 0 
    sub_nodes = []
    
    
    st = steps.copy()
    #st = steps
    
    split_data,split_regression,column,st = find_best_split(X,labels,feature_ids,st)
    #print(column)
    print(depth)
    print(len(split_data))
    print(st)
    #print("///////////////////////////////////////////////////////////////////////////////////////////////////")
    for i,j in zip(split_data,split_regression):
        # len(i[0]) == 0 problem ???
        #print(i[0])
        #print(i[0])
        
        node = Node(i[1],i[0],feature_ids)#no need for feature_ids -> object inheritance   
        node.split = column   #column
        
        #split point
       
        node.split_result = i[0][-1][column]
        node.depth = depth
        node.regr = j
        
        node.top = i[0][-1][column]
        node.bottom = i[0][0][column]
        node.steps = st
        #print(node.steps)
        
        
        #Ha egy bizonyos error érték alá megyünk vagy elértünk egy bizonyos elemszámot akkor a node-ot leaf-nek nyilvánítjuk
        if(len(node.labels) <= leaf_size or calculate_error(node.regr,node.X,node.labels,split,0,3)<1):#############################
            node.leaf = 1
            #print(node.X)
            #print(node.split)
            #print("leaf")
            
            
        else:
            node.leaf = 0
        sub_nodes.append(node)
    

    if(depth == 2000):
        for i in sub_nodes:
            i.leaf = 1
            print("leaf2")
        return sub_nodes
    
    depth +=1
    #############################
    ''''n = 0
    for i in steps:
        if(i > 4):
            n+=1
    
    if(n == len(steps)):
        return sub_nodes'''
    ############################
    
    leaf =  0
    for i in sub_nodes:
        #print(i.steps)
        if(i.leaf == 1):
            leaf +=1
    #if we reach limit return nodes 
            
            
    if(leaf == len(sub_nodes)):
        '''print("leaf3")
        print(X)
        print("*******")
        print(split_data)
        print(steps)'''
        return sub_nodes
    
    else:
        for node in sub_nodes:

            if(node.leaf == 0):
                
                node.nodes = build_tree(node.X,node.feature_ids,node.labels,leaf_size,var,depth,node.steps)


    return sub_nodes   


